#!/usr/bin/env python3
"""
GPT-4o-minië¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
"""
import json
import os
from openai import OpenAI
from typing import List, Dict
import time
from tqdm import tqdm

def initialize_client():
    """Initialize OpenAI client with API key from environment variable"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        exit(1)

    client = OpenAI(api_key=api_key)
    return client

def generate_typos_batch(client, entries: List[Dict], batch_size: int = 5) -> List[Dict]:
    """í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜¤íƒ€ ìƒì„± ê·œì¹™:
1. êµì²´(Substitution): ã…â†”ã…”, ã„¹â†”ã„´ ë“± ìëª¨ë‚˜ ë°œìŒì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ êµì²´
2. ì‚­ì œ(Deletion): ìëª¨ ë˜ëŠ” ìŒì ˆ ë‹¨ìœ„ ëˆ„ë½
3. ì¶”ê°€(Insertion): ë¶ˆí•„ìš”í•œ ìëª¨ë‚˜ ìŒì ˆ ì‚½ì…
4. ì „ì¹˜(Transposition): ìëª¨ë‚˜ ìŒì ˆ ìˆœì„œ ë°”ë€œ
5. ë„ì–´ì“°ê¸° ì˜¤ë¥˜(Spacing): ë¶ˆí•„ìš”í•œ ê³µë°± ì¶”ê°€ ë˜ëŠ” ê³µë°± ëˆ„ë½

ê° ë¬¸ì¥ì— ëŒ€í•´ 3ê°œ ë²„ì „ì„ ìƒì„±:
- Case 1: ì˜¤íƒ€ ì—†ìŒ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
- Case 2: ì˜¤íƒ€ 1ê°œ (ê·œì¹™ ì¤‘ 1ê°œ ì ìš©)
- Case 3: ì˜¤íƒ€ 2ê°œ (ê·œì¹™ ì¤‘ 2ê°œ ì ìš©)

ì£¼ì˜ì‚¬í•­:
- ì˜¤íƒ€ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì‹¤ìˆ˜ì—¬ì•¼ í•¨
- ë¬¸ì¥ì˜ ì˜ë¯¸ëŠ” ìœ ì¶” ê°€ëŠ¥í•´ì•¼ í•¨
- ì˜ì–´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€

ì…ë ¥ ë°ì´í„°:
{batch_text}

ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´):
[
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0, "error_types": []}},
      {{"text": "ì˜¤íƒ€ 1ê°œ ë²„ì „", "num_errors": 1, "error_types": ["ì‚¬ìš©ëœ ì˜¤íƒ€ ìœ í˜•"]}},
      {{"text": "ì˜¤íƒ€ 2ê°œ ë²„ì „", "num_errors": 2, "error_types": ["ìœ í˜•1", "ìœ í˜•2"]}}
    ]
  }},
  â€¦
]

JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ ëª…í™•í•˜ê³  êµ¬ë¶„ ê°€ëŠ¥í•œ ì˜¤íƒ€ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )

        # ì‘ë‹µ íŒŒì‹±
        result_text = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"âŒ JSON parsing error: {e}")
        print(f"Response: {result_text[:200]}...")
        return []
    except Exception as e:
        print(f"âŒ API error: {e}")
        return []

def process_dataset(input_file: str, output_file: str, batch_size: int = 5):
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì˜¤íƒ€ ìƒì„±"""

    print(f"ğŸ“š Loading {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
    simple_data = []
    for entry in data:
        if "query" in entry:
            simple_data.append({
                "en": entry["query"],
                "ko": entry["queries"]["ko"]
            })
        else:
            simple_data.append(entry)

    print(f"ğŸ“Š Total entries to process: {len(simple_data)}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = initialize_client()

    # ë°°ì¹˜ ì²˜ë¦¬
    all_results = []
    error_types = ['substitution', 'deletion', 'insertion', 'transposition', 'spacing']

    print(f"ğŸ”„ Processing in batches of {batch_size}...")

    for i in tqdm(range(0, len(simple_data), batch_size), desc="Processing batches"):
        batch = simple_data[i:i+batch_size]

        # GPTë¡œ ì˜¤íƒ€ ìƒì„±
        batch_results = generate_typos_batch(client, batch, batch_size)

        # ê²°ê³¼ë¥¼ í”Œë« í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for entry_group in batch_results:
            en_query = entry_group.get("en", "")
            ko_original = entry_group.get("ko_original", "")
            error_type = entry_group.get("error_type", "")

            for variant in entry_group.get("variants", []):
                flat_entry = {
                    "en": en_query,
                    "ko_original": ko_original,
                    "ko_typo": variant["text"],
                    "error_type": error_type,
                    "num_errors": variant["num_errors"],
                    "applied_errors": [error_type] * variant["num_errors"] if variant["num_errors"] > 0 else []
                }
                all_results.append(flat_entry)

        # Rate limiting
        if i + batch_size < len(simple_data):
            time.sleep(1)

    print(f"\nâœ… Generated {len(all_results)} entries")

    # ê²°ê³¼ ì €ì¥
    print(f"ğŸ’¾ Saving to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    print(f"  Original entries: {len(simple_data)}")
    print(f"  Generated entries: {len(all_results)}")

    # ê° ì˜¤íƒ€ ìœ í˜•ë³„ ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“ Sample outputs by error type:")
    for error_type in error_types:
        type_entries = [e for e in all_results if e['error_type'] == error_type]
        if len(type_entries) >= 3:
            print(f"\n--- {error_type.upper()} ---")
            print(f"Original: {type_entries[0]['ko_original']}")
            print(f"0 errors: {type_entries[0]['ko_typo']}")
            print(f"1 error:  {type_entries[1]['ko_typo']}")
            print(f"2 errors: {type_entries[2]['ko_typo']}")

    return len(all_results)

def create_test_sample(input_file: str, sample_file: str, sample_size: int = 5):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŒŒì¼ ìƒì„±"""
    print(f"ğŸ“ Creating sample file with {sample_size} entries...")

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    sample_data = data[:sample_size]

    with open(sample_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… Sample file created: {sample_file}")
    return sample_file

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate typos using GPT-4o-mini")
    parser.add_argument("--input", default="mkqa_formatted.json", help="Input JSON file")
    parser.add_argument("--output", default="mkqa_with_typos_gpt.json", help="Output JSON file")
    parser.add_argument("--batch-size", type=int, default=3, help="Batch size for API calls")
    parser.add_argument("--test", action="store_true", help="Test with small sample first")
    parser.add_argument("--sample-size", type=int, default=5, help="Sample size for testing")

    args = parser.parse_args()

    if args.test:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        sample_file = "mkqa_typo_sample.json"
        create_test_sample(args.input, sample_file, args.sample_size)

        print("\nğŸ§ª Testing with sample file...")
        count = process_dataset(
            sample_file,
            "mkqa_typo_sample_output.json",
            batch_size=2
        )
        print(f"\nâœ… Test complete! Generated {count} entries")
        print("Check 'mkqa_typo_sample_output.json' for results")
    else:
        # ì „ì²´ ì²˜ë¦¬
        count = process_dataset(
            args.input,
            args.output,
            batch_size=args.batch_size
        )
        print(f"\nâœ… Complete! Generated {count} entries")
        print(f"Output saved to: {args.output}")