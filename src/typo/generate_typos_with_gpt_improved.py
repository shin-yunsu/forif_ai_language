#!/usr/bin/env python3
"""
GPT-4o-minië¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
"""
import json
import os
from openai import OpenAI
from typing import List, Dict
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue

def initialize_client():
    """Initialize OpenAI client with API key from environment variable"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        exit(1)

    client = OpenAI(api_key=api_key)
    return client

def generate_typos_batch(client, entries: List[Dict], batch_size: int = 5) -> List[Dict]:
    """GPTë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤íƒ€ ìƒì„±"""

    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
    batch_text = json.dumps(entries, ensure_ascii=False, indent=2)

    prompt = f"""í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜¤íƒ€ ìƒì„± ê·œì¹™ê³¼ ì˜ˆì‹œ:

1. êµì²´(Substitution): ìëª¨ë‚˜ ë°œìŒì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ êµì²´
   - ëª¨ìŒ êµì²´: ã…â†”ã…”, ã…“â†”ã…, ã…—â†”ã…œ, ã…¡â†”ã…“, ã…£â†”ã…¡
   - ììŒ êµì²´: ã„¹â†”ã„´, ã…‚â†”ã…, ã„±â†”ã…‹, ã„·â†”ã…Œ, ã…ˆâ†”ã…Š, ã……â†”ã…†
   ì˜ˆì‹œ: "ì–´ë””ì—ì„œ" â†’ "ì–´ë””ì• ì„œ" (ã…”â†’ã…)
   ì˜ˆì‹œ: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ë¼ìš”" (ã„´â†’ã„¹)

2. ì‚­ì œ(Deletion): ìëª¨ ë˜ëŠ” ìŒì ˆ ë‹¨ìœ„ ëˆ„ë½
   - ìŒì ˆ ì‚­ì œ: "ì–´ë””ì—ì„œ" â†’ "ì–´ë””ì„œ" (ì— ì‚­ì œ)
   - ìëª¨ ì‚­ì œ: "í–ˆìŠµë‹ˆë‹¤" â†’ "í–ˆìŠµë‹ˆë‹¤" (ì¢…ì„± ã…† ì‚­ì œ â†’ "í–ˆìŠ¤ë‹ˆë‹¤")
   ì˜ˆì‹œ: "ì¸êµ¬ëŠ”?" â†’ "ì¸êµ¬ã…¡ã„´" (ã„´ ì‚­ì œ)
   ì˜ˆì‹œ: "ì„œìš¸" â†’ "ì„œ" (ìš¸ ì‚­ì œ)

3. ì¶”ê°€(Insertion): ë¶ˆí•„ìš”í•œ ìëª¨ë‚˜ ìŒì ˆ ì‚½ì…
   - ìŒì ˆ ì¤‘ë³µ: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ë²…ìŠ¤ìŠ¤" (ìŠ¤ ì¤‘ë³µ)
   - ìëª¨ ì¶”ê°€: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ã……ë‚˜ìš”" (ã…… ì¶”ê°€)
   ì˜ˆì‹œ: "ë¡œê³ ëŠ”" â†’ "ë¡œê³ ê³ ëŠ”" (ê³  ì¤‘ë³µ)
   ì˜ˆì‹œ: "ì–´ë””" â†’ "ì–´ë””ã…£" (ã…£ ì¶”ê°€)

4. ì „ì¹˜(Transposition): ì¸ì ‘í•œ ìëª¨ë‚˜ ìŒì ˆ ìˆœì„œ ë°”ë€œ
   - ìŒì ˆ ì „ì¹˜: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤ë²…íƒ€ìŠ¤" (íƒ€ë²… ìˆœì„œ ë³€ê²½)
   - ìëª¨ ì „ì¹˜: "ì„œìš¸" â†’ "ì„œã…œã…‡ã„¹" (ã…‡,ã…œ ìˆœì„œ ë³€ê²½)
   ì˜ˆì‹œ: "ë¡œê³ ëŠ”" â†’ "ê³ ë¡œëŠ”" (ë¡œê³  ìˆœì„œ ë³€ê²½)
   ì˜ˆì‹œ: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ã…ã„´ìš”" (ã„´,ã… ìˆœì„œ ë³€ê²½)

5. ë„ì–´ì“°ê¸° ì˜¤ë¥˜(Spacing): ê³µë°± ì¶”ê°€/ì œê±°
   - ê³µë°± ì œê±°: "ì–´ë””ì—ì„œ ë‚˜ì™”ë‚˜ìš”" â†’ "ì–´ë””ì—ì„œë‚˜ì™”ë‚˜ìš”"
   - ê³µë°± ì¶”ê°€: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ ë²…ìŠ¤"
   ì˜ˆì‹œ: "ê·¸ë“¤ë§Œì˜ ë¦¬ê·¸" â†’ "ê·¸ë“¤ë§Œì˜ë¦¬ê·¸" (ê³µë°± ì œê±°)
   ì˜ˆì‹œ: "ë¼ì´ì˜¨í‚¹" â†’ "ë¼ì´ì˜¨ í‚¹" (ê³µë°± ì¶”ê°€)

ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ 2ê°œ ë²„ì „ ìƒì„±:
- 1 error: í•´ë‹¹ ì˜¤íƒ€ ìœ í˜• 1ë²ˆ ì ìš©
- 2 errors: í•´ë‹¹ ì˜¤íƒ€ ìœ í˜• 2ë²ˆ ì ìš© (1 error ë²„ì „ì— ì¶”ê°€í•˜ì—¬)

ì…ë ¥ ë°ì´í„°:
{batch_text}

ì¶œë ¥ í˜•ì‹:
[{{
  "original": "ì›ë³¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸",
  "substitution": {{
    "1_error": "êµì²´ ì˜¤íƒ€ í•˜ë‚˜",
    "2_errors": "êµì²´ ì˜¤íƒ€ ë‘˜"
  }},
  "deletion": {{
    "1_error": "ì‚­ì œ ì˜¤íƒ€ í•˜ë‚˜",
    "2_errors": "ì‚­ì œ ì˜¤íƒ€ ë‘˜"
  }},
  "insertion": {{
    "1_error": "ì¶”ê°€ ì˜¤íƒ€ í•˜ë‚˜",
    "2_errors": "ì¶”ê°€ ì˜¤íƒ€ ë‘˜"
  }},
  "transposition": {{
    "1_error": "ì „ì¹˜ ì˜¤íƒ€ í•˜ë‚˜",
    "2_errors": "ì „ì¹˜ ì˜¤íƒ€ ë‘˜"
  }},
  "spacing": {{
    "1_error": "ë„ì–´ì“°ê¸° ì˜¤íƒ€ í•˜ë‚˜",
    "2_errors": "ë„ì–´ì“°ê¸° ì˜¤íƒ€ ë‘˜"
  }}
}}]

JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ ëª…í™•í•˜ê³  êµ¬ë¶„ ê°€ëŠ¥í•œ ì˜¤íƒ€ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )

        # ì‘ë‹µ íŒŒì‹±
        result_text = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"âŒ JSON parsing error: {e}")
        print(f"Response: {result_text[:200]}...")
        return []
    except Exception as e:
        print(f"âŒ API error: {e}")
        return []

def process_dataset(input_file: str, output_file: str, batch_size: int = 5, max_workers: int = 5):
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì˜¤íƒ€ ìƒì„± (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)"""

    print(f"ğŸ“š Loading {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
    simple_data = []
    for entry in data:
        if "query" in entry:
            simple_data.append({
                "en": entry["query"],
                "ko": entry["queries"]["ko"]
            })
        else:
            simple_data.append(entry)

    print(f"ğŸ“Š Total entries to process: {len(simple_data)}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = initialize_client()

    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
    all_results = []
    results_lock = threading.Lock()  # Thread-safe ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ lock
    error_types = ['substitution', 'deletion', 'insertion', 'transposition', 'spacing']

    print(f"ğŸ”„ Processing in batches of {batch_size} with {max_workers} workers...")

    # ë°°ì¹˜ ì‘ì—…ì„ ìœ„í•œ í•¨ìˆ˜
    def process_batch_worker(batch_data, batch_index):
        """ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜"""
        try:
            # GPTë¡œ ì˜¤íƒ€ ìƒì„±
            batch_results = generate_typos_batch(client, batch_data, batch_size)

            # ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (í”Œë« ë³€í™˜ ì—†ìŒ)
            batch_flat_results = batch_results

            return batch_index, batch_flat_results
        except Exception as e:
            print(f"âŒ Error processing batch {batch_index}: {e}")
            return batch_index, []

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        # ë°°ì¹˜ ì‘ì—… ì œì¶œ
        for i in range(0, len(simple_data), batch_size):
            batch = simple_data[i:i+batch_size]
            batch_index = i // batch_size
            future = executor.submit(process_batch_worker, batch, batch_index)
            futures[future] = batch_index

        # ê²°ê³¼ ìˆ˜ì§‘ (ìˆœì„œ ë³´ì¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
        results_dict = {}

        # Progress barì™€ í•¨ê»˜ ê²°ê³¼ ìˆ˜ì§‘
        with tqdm(total=len(futures), desc="Processing batches") as pbar:
            for future in as_completed(futures):
                batch_index, batch_results = future.result()
                results_dict[batch_index] = batch_results
                pbar.update(1)

        # ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ë³‘í•©
        for i in sorted(results_dict.keys()):
            all_results.extend(results_dict[i])

    print(f"\nâœ… Generated {len(all_results)} entries")

    # ê²°ê³¼ ì €ì¥
    print(f"ğŸ’¾ Saving to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    print(f"  Original entries: {len(simple_data)}")
    print(f"  Generated entries: {len(all_results)}")

    # ìƒ˜í”Œ ì¶œë ¥
    if all_results:
        print("\nğŸ“ Sample output:")
        sample = all_results[0]
        print(f"Original: {sample.get('original', '')}")
        for error_type in error_types:
            if error_type in sample:
                print(f"\n{error_type.upper()}:")
                if '1_error' in sample[error_type]:
                    print(f"  1 error: {sample[error_type]['1_error']}")
                if '2_errors' in sample[error_type]:
                    print(f"  2 errors: {sample[error_type]['2_errors']}")

    return len(all_results)

def create_test_sample(input_file: str, sample_file: str, sample_size: int = 5):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŒŒì¼ ìƒì„±"""
    print(f"ğŸ“ Creating sample file with {sample_size} entries...")

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    sample_data = data[:sample_size]

    with open(sample_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… Sample file created: {sample_file}")
    return sample_file

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate typos using GPT-4o-mini")
    parser.add_argument("--input", default="mkqa_refined_simple.json", help="Input JSON file")
    parser.add_argument("--output", default="mkqa_with_typos_gpt.json", help="Output JSON file")
    parser.add_argument("--batch-size", type=int, default=3, help="Batch size for API calls")
    parser.add_argument("--max-workers", type=int, default=5, help="Maximum number of parallel workers")
    parser.add_argument("--test", action="store_true", help="Test with small sample first")
    parser.add_argument("--sample-size", type=int, default=5, help="Sample size for testing")

    args = parser.parse_args()

    if args.test:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        sample_file = "mkqa_typo_sample.json"
        create_test_sample(args.input, sample_file, args.sample_size)

        print("\nğŸ§ª Testing with sample file...")
        count = process_dataset(
            sample_file,
            "mkqa_typo_sample_output.json",
            batch_size=2,
            max_workers=2
        )
        print(f"\nâœ… Test complete! Generated {count} entries")
        print("Check 'mkqa_typo_sample_output.json' for results")
    else:
        # ì „ì²´ ì²˜ë¦¬
        count = process_dataset(
            args.input,
            args.output,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )
        print(f"\nâœ… Complete! Generated {count} entries")
        print(f"Output saved to: {args.output}")