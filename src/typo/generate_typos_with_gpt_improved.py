#!/usr/bin/env python3
"""
GPT-4o-minië¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
"""
import json
import os
from openai import OpenAI
from typing import List, Dict
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue

def initialize_client():
    """Initialize OpenAI client with API key from environment variable"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        exit(1)

    client = OpenAI(api_key=api_key)
    return client

def generate_typos_batch(client, entries: List[Dict], batch_size: int = 5) -> List[Dict]:
    """GPTë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤íƒ€ ìƒì„±"""

    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
    # batch_text = json.dumps(entries, ensure_ascii=False, indent=2)
    korean_texts = [entry["ko"] for entry in entries]
    batch_text = json.dumps(korean_texts, ensure_ascii=False, indent=2)

    prompt = f"""
í•œêµ­ì–´ ë¬¸ì¥ì— ì˜ë„ì ì¸ ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ ì •í™•íˆ 1ê°œ ë˜ëŠ” 2ê°œì˜ ì˜¤ë¥˜ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

## ì˜¤íƒ€ ìƒì„± ê·œì¹™

### 1. êµì²´(Substitution) - ë¹„ìŠ·í•œ ìëª¨ë¥¼ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ë°”ê¾¸ê¸°
- **ëª¨ìŒ êµì²´ ê°€ëŠ¥ ìŒ**: ã…â†”ã…”, ã…“â†”ã…, ã…—â†”ã…œ, ã…¡â†”ã…“, ã…£â†”ã…¡, ã…•â†”ã…“, ã…›â†”ã…—, ã… â†”ã…œ
- **ììŒ êµì²´ ê°€ëŠ¥ ìŒ**: ã„¹â†”ã„´, ã…‚â†”ã…, ã„±â†”ã…‹, ã„·â†”ã…Œ, ã…ˆâ†”ã…Š, ã……â†”ã…†, ã…‡â†”ã„¹, ã…ƒâ†”ã…‚, ã„¸â†”ã„·
- ì˜ˆ: "ê°€ì¥" â†’ "ê°€ì˜" (ã…‡â†’ã„¹)

### 2. ì‚­ì œ(Deletion) - ìëª¨ ë˜ëŠ” ìŒì ˆ ë¹¼ê¸°
- **ìëª¨ ì‚­ì œ**: ë°›ì¹¨ì´ë‚˜ ëª¨ìŒ ì¼ë¶€ë¥¼ ì œê±°
- **ìŒì ˆ ì‚­ì œ**: ì „ì²´ ê¸€ìë¥¼ ì œê±°
- ì˜ˆ: "ê°€ì¥" â†’ "ê°€ã…ˆã…‡" (ëª¨ìŒ ã… ì‚­ì œ) ë˜ëŠ” "ê°€" (ìŒì ˆ ì‚­ì œ)

### 3. ì¶”ê°€(Insertion) - ë¶ˆí•„ìš”í•œ ìëª¨ë‚˜ ìŒì ˆ ë„£ê¸°
- **ìëª¨ ì¶”ê°€**: ë°›ì¹¨ì´ë‚˜ ëª¨ìŒì„ ì¶”ê°€
- **ìŒì ˆ ì¶”ê°€**: ì „ì²´ ê¸€ìë¥¼ ì¶”ê°€
- ì˜ˆ: "ê°€ì¥" â†’ "ê°€ì¥ã…‡" (ìëª¨ ì¶”ê°€) ë˜ëŠ” "ê°€ì¥ì¥" (ìŒì ˆ ì¶”ê°€)

### 4. ì „ì¹˜(Transposition) - ì¸ì ‘í•œ ìš”ì†Œì˜ ìˆœì„œ ë°”ê¾¸ê¸°
- **ìëª¨ ì „ì¹˜**: í•œ ê¸€ì ë‚´ì—ì„œ ìëª¨ ìˆœì„œ ë³€ê²½
- **ìŒì ˆ ì „ì¹˜**: ì¸ì ‘í•œ ë‘ ê¸€ìì˜ ìˆœì„œ ë³€ê²½
- ì˜ˆ: "ê°€ì¥" â†’ "ê°€ã…ã…ˆã…‡" (ìëª¨ ì „ì¹˜) ë˜ëŠ” "ì¥ê°€" (ìŒì ˆ ì „ì¹˜)

### 5. ë„ì–´ì“°ê¸°(Spacing) - ê³µë°± ì¶”ê°€ ë˜ëŠ” ì œê±°
- **ê³µë°± ì œê±°**: ë„ì–´ì“°ê¸° ì‚­ì œ
- **ê³µë°± ì¶”ê°€**: ë‹¨ì–´ ì¤‘ê°„ì— ë¶ˆí•„ìš”í•œ ê³µë°± ì‚½ì…
- ì˜ˆ: "ê°€ì¥ í°" â†’ "ê°€ì¥í°" (ê³µë°± ì œê±°) ë˜ëŠ” "ê°€ ì¥ í°" (ê³µë°± ì¶”ê°€)

## ì¤‘ìš” ê·œì¹™
1. **ì˜¤ë¥˜ ê°œìˆ˜ ì—„ìˆ˜**: 
   - "1_error": í•´ë‹¹ ìœ í˜•ì˜ ì˜¤íƒ€ë¥¼ ì •í™•íˆ 1ê°œë§Œ ì ìš©
   - "2_errors": í•´ë‹¹ ìœ í˜•ì˜ ì˜¤íƒ€ë¥¼ ì •í™•íˆ 2ê°œ ì ìš© (1ì—ì„œ ì¶”ê°€í•˜ì—¬)

2. **ì›ë¬¸ ë³´ì¡´**: ì˜¤íƒ€ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ì›ë¬¸ê³¼ ë™ì¼í•´ì•¼ í•¨

3. **ìì—°ìŠ¤ëŸ¬ìš´ ì˜¤íƒ€**: ì‹¤ì œ íƒ€ì´í•‘ ì‹¤ìˆ˜ì²˜ëŸ¼ ë³´ì´ë„ë¡ ìƒì„±

## ì¶œë ¥ í˜•ì‹
ê° ë¬¸ì¥ì— ëŒ€í•´ ë‹¤ìŒ JSON êµ¬ì¡°ë¡œ ì¶œë ¥:
[{{
  "original": "ì›ë³¸ ë¬¸ì¥",
  "substitution": {{
    "1_error": "êµì²´ ì˜¤íƒ€ 1ê°œ ì ìš©ëœ ë¬¸ì¥",
    "2_errors": "êµì²´ ì˜¤íƒ€ 2ê°œ ì ìš©ëœ ë¬¸ì¥"
  }},
  "deletion": {{
    "1_error": "ì‚­ì œ ì˜¤íƒ€ 1ê°œ ì ìš©ëœ ë¬¸ì¥",
    "2_errors": "ì‚­ì œ ì˜¤íƒ€ 2ê°œ ì ìš©ëœ ë¬¸ì¥"
  }},
  "insertion": {{
    "1_error": "ì¶”ê°€ ì˜¤íƒ€ 1ê°œ ì ìš©ëœ ë¬¸ì¥",
    "2_errors": "ì¶”ê°€ ì˜¤íƒ€ 2ê°œ ì ìš©ëœ ë¬¸ì¥"
  }},
  "transposition": {{
    "1_error": "ì „ì¹˜ ì˜¤íƒ€ 1ê°œ ì ìš©ëœ ë¬¸ì¥",
    "2_errors": "ì „ì¹˜ ì˜¤íƒ€ 2ê°œ ì ìš©ëœ ë¬¸ì¥"
  }},
  "spacing": {{
    "1_error": "ë„ì–´ì“°ê¸° ì˜¤íƒ€ 1ê°œ ì ìš©ëœ ë¬¸ì¥",
    "2_errors": "ë„ì–´ì“°ê¸° ì˜¤íƒ€ 2ê°œ ì ìš©ëœ ë¬¸ì¥"
  }}
}}]

## ì‹¤ì œ ì˜ˆì‹œ
ì…ë ¥: "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€"
ì¶œë ¥:
{{
  "original": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€",
  "substitution": {{
    "1_error": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë‹¤ì¸ê°€",
     "2_errors": "ë¯¸êµ­ì˜ ìˆ˜ë‘ëŠ” ì–´ë‹¤ì¸ê°€"  
  }},
  "deletion": {{
    "1_error": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ê°€",  
    "2_errors": "ë¯¸êµ­ì˜ ìˆ˜ë„ã…¡ã„´ ì–´ë””ê°€"
  }},
  "insertion": {{
    "1_error": "ë¯¸êµ­ì˜ ìˆ˜ë„ë„ëŠ” ì–´ë””ì¸ê°€", 
    "2_errors": "ë¯¸êµ­ì˜ ìˆ˜ë„ë„ëŠ” ì–´ë””ì¸ê°€ã…"
  }},
  "transposition": {{
    "1_error": "ë¯¸êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ì¸ë””ê°€",  
    "2_errors": "ë¯¸êµ­ì˜ ë„ìˆ˜ëŠ” ì–´ë””ã…£ã…‡ã„´ê°€"
  }},
  "spacing": {{
    "1_error": "ë¯¸êµ­ì˜ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€",  
    "2_errors": "ë¯¸êµ­ì˜ìˆ˜ë„ëŠ” ì–´ ë””ì¸ê°€" 
  }}
}}

ì…ë ¥ í…ìŠ¤íŠ¸:
{batch_text}

ìœ„ ê·œì¹™ê³¼ í˜•ì‹ì— ë”°ë¼ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì˜ˆì‹œì™€ ê°™ì€ íŒ¨í„´ê³¼ í˜•ì‹ìœ¼ë¡œ ì˜¤íƒ€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. JSON í˜•ì‹ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )

        # ì‘ë‹µ íŒŒì‹±
        result_text = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"âŒ JSON parsing error: {e}")
        print(f"Response: {result_text[:200]}...")
        return []
    except Exception as e:
        print(f"âŒ API error: {e}")
        return []

def process_dataset(input_file: str, output_file: str, batch_size: int = 5, max_workers: int = 5):
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì˜¤íƒ€ ìƒì„± (ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›)"""

    print(f"ğŸ“š Loading {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
    simple_data = []
    
    # ì…ë ¥ì´ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
    if isinstance(data, list) and all(isinstance(item, str) for item in data):
        for text in data:
            simple_data.append({"ko": text})
    # ê¸°ì¡´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë„ ì§€ì›
    else:
        for entry in data:
            if isinstance(entry, dict):
                if "ko" in entry:
                    simple_data.append({
                        "en": entry.get("en", ""),
                        "ko": entry["ko"]
                    })
                elif "query" in entry and "queries" in entry and "ko" in entry["queries"]:
                    simple_data.append({
                        "en": entry["query"],
                        "ko": entry["queries"]["ko"]
                    })
            elif isinstance(entry, str):
                simple_data.append({"ko": entry})

    print(f"ğŸ“Š Total entries to process: {len(simple_data)}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = initialize_client()

    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¤€ë¹„
    all_results = []
    results_lock = threading.Lock()  # Thread-safe ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ lock
    error_types = ['substitution', 'deletion', 'insertion', 'transposition', 'spacing']

    print(f"ğŸ”„ Processing in batches of {batch_size} with {max_workers} workers...")

    # ë°°ì¹˜ ì‘ì—…ì„ ìœ„í•œ í•¨ìˆ˜
    def process_batch_worker(batch_data, batch_index):
        """ê°œë³„ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜"""
        try:
            # GPTë¡œ ì˜¤íƒ€ ìƒì„±
            batch_results = generate_typos_batch(client, batch_data, batch_size)

            # ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (í”Œë« ë³€í™˜ ì—†ìŒ)
            batch_flat_results = batch_results

            return batch_index, batch_flat_results
        except Exception as e:
            print(f"âŒ Error processing batch {batch_index}: {e}")
            return batch_index, []

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}

        # ë°°ì¹˜ ì‘ì—… ì œì¶œ
        for i in range(0, len(simple_data), batch_size):
            batch = simple_data[i:i+batch_size]
            batch_index = i // batch_size
            future = executor.submit(process_batch_worker, batch, batch_index)
            futures[future] = batch_index

        # ê²°ê³¼ ìˆ˜ì§‘ (ìˆœì„œ ë³´ì¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©)
        results_dict = {}

        # Progress barì™€ í•¨ê»˜ ê²°ê³¼ ìˆ˜ì§‘
        with tqdm(total=len(futures), desc="Processing batches") as pbar:
            for future in as_completed(futures):
                batch_index, batch_results = future.result()
                results_dict[batch_index] = batch_results
                pbar.update(1)

        # ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ë³‘í•©
        for i in sorted(results_dict.keys()):
            all_results.extend(results_dict[i])

    print(f"\nâœ… Generated {len(all_results)} entries")

    # ê²°ê³¼ ì €ì¥
    print(f"ğŸ’¾ Saving to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    print(f"  Original entries: {len(simple_data)}")
    print(f"  Generated entries: {len(all_results)}")

    # ìƒ˜í”Œ ì¶œë ¥
    if all_results:
        print("\nğŸ“ Sample output:")
        sample = all_results[0]
        print(f"Original: {sample.get('original', '')}")
        for error_type in error_types:
            if error_type in sample:
                print(f"\n{error_type.upper()}:")
                if '1_error' in sample[error_type]:
                    print(f"  1 error: {sample[error_type]['1_error']}")
                if '2_errors' in sample[error_type]:
                    print(f"  2 errors: {sample[error_type]['2_errors']}")

    return len(all_results)

def create_test_sample(input_file: str, sample_file: str, sample_size: int = 5):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŒŒì¼ ìƒì„±"""
    print(f"ğŸ“ Creating sample file with {sample_size} entries...")

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    sample_data = data[:sample_size]

    with open(sample_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… Sample file created: {sample_file}")
    return sample_file

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate typos using GPT-4o-mini")
    parser.add_argument("--input", default="mkqa_refined_simple.json", help="Input JSON file")
    parser.add_argument("--output", default="mkqa_with_typos_gpt.json", help="Output JSON file")
    parser.add_argument("--batch-size", type=int, default=3, help="Batch size for API calls")
    parser.add_argument("--max-workers", type=int, default=5, help="Maximum number of parallel workers")
    parser.add_argument("--test", action="store_true", help="Test with small sample first")
    parser.add_argument("--sample-size", type=int, default=5, help="Sample size for testing")

    args = parser.parse_args()

    if args.test:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        sample_file = "mkqa_typo_sample.json"
        create_test_sample(args.input, sample_file, args.sample_size)

        print("\nğŸ§ª Testing with sample file...")
        count = process_dataset(
            sample_file,
            "mkqa_typo_sample_output.json",
            batch_size=2,
            max_workers=2
        )
        print(f"\nâœ… Test complete! Generated {count} entries")
        print("Check 'mkqa_typo_sample_output.json' for results")
    else:
        # ì „ì²´ ì²˜ë¦¬
        count = process_dataset(
            args.input,
            args.output,
            batch_size=args.batch_size,
            max_workers=args.max_workers
        )
        print(f"\nâœ… Complete! Generated {count} entries")
        print(f"Output saved to: {args.output}")