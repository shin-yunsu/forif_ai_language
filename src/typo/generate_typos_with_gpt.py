#!/usr/bin/env python3
"""
GPT-4o-minië¥¼ ì‚¬ìš©í•´ì„œ í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
"""
import json
import os
from openai import OpenAI
from typing import List, Dict
import time
from tqdm import tqdm

def initialize_client():
    """Initialize OpenAI client with API key from environment variable"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  Please set OPENAI_API_KEY environment variable")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        exit(1)

    client = OpenAI(api_key=api_key)
    return client

def generate_typos_batch(client, entries: List[Dict], batch_size: int = 5) -> List[Dict]:
    """GPTë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤íƒ€ ìƒì„±"""

    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
    batch_text = json.dumps(entries, ensure_ascii=False, indent=2)

    prompt = f"""í•œêµ­ì–´ ë¬¸ì¥ì— ì˜¤íƒ€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜¤íƒ€ ìƒì„± ê·œì¹™ê³¼ ì˜ˆì‹œ:

1. êµì²´(Substitution): ìëª¨ë‚˜ ë°œìŒì´ ë¹„ìŠ·í•œ ê²ƒë¼ë¦¬ êµì²´
   - ëª¨ìŒ êµì²´: ã…â†”ã…”, ã…“â†”ã…, ã…—â†”ã…œ, ã…¡â†”ã…“, ã…£â†”ã…¡
   - ììŒ êµì²´: ã„¹â†”ã„´, ã…‚â†”ã…, ã„±â†”ã…‹, ã„·â†”ã…Œ, ã…ˆâ†”ã…Š, ã……â†”ã…†
   ì˜ˆì‹œ: "ì–´ë””ì—ì„œ" â†’ "ì–´ë””ì• ì„œ" (ã…”â†’ã…)
   ì˜ˆì‹œ: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ë¼ìš”" (ã„´â†’ã„¹)

2. ì‚­ì œ(Deletion): ìëª¨ ë˜ëŠ” ìŒì ˆ ë‹¨ìœ„ ëˆ„ë½
   - ìŒì ˆ ì‚­ì œ: "ì–´ë””ì—ì„œ" â†’ "ì–´ë””ì„œ" (ì— ì‚­ì œ)
   - ìëª¨ ì‚­ì œ: "í–ˆìŠµë‹ˆë‹¤" â†’ "í–ˆìŠµë‹ˆë‹¤" (ì¢…ì„± ã…† ì‚­ì œ â†’ "í–ˆìŠ¤ë‹ˆë‹¤")
   ì˜ˆì‹œ: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ë²…" (ìŠ¤ ì‚­ì œ)
   ì˜ˆì‹œ: "ê·¸ë“¤ë§Œì˜" â†’ "ê·¸ë“¤ì˜" (ë§Œ ì‚­ì œ)

3. ì¶”ê°€(Insertion): ë¶ˆí•„ìš”í•œ ìëª¨ë‚˜ ìŒì ˆ ì‚½ì…
   - ìŒì ˆ ì¤‘ë³µ: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ë²…ë²…ìŠ¤" (ë²… ì¤‘ë³µ)
   - ìëª¨ ì¶”ê°€: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ì—ˆë‚˜ìš”" (ì—ˆ ì¶”ê°€)
   ì˜ˆì‹œ: "ë¡œê³ ëŠ”" â†’ "ë¡œê³ ê³ ëŠ”" (ê³  ì¤‘ë³µ)
   ì˜ˆì‹œ: "ì–´ë””" â†’ "ì–´ë””ì´" (ì´ ì¶”ê°€)

4. ì „ì¹˜(Transposition): ì¸ì ‘í•œ ìëª¨ë‚˜ ìŒì ˆ ìˆœì„œ ë°”ë€œ
   - ìŒì ˆ ì „ì¹˜: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "íƒ€ìŠ¤ë²…ìŠ¤" (ìŠ¤íƒ€ ìˆœì„œ ë³€ê²½)
   - ìëª¨ ì „ì¹˜: "ì–´ë””" â†’ "ì–´ë””" (ì´ˆì„± êµí™˜ì€ ì–´ë ¤ìš°ë¯€ë¡œ ìŒì ˆ ë‹¨ìœ„ ê¶Œì¥)
   ì˜ˆì‹œ: "ë¡œê³ ëŠ”" â†’ "ê³ ë¡œëŠ”" (ë¡œê³  ìˆœì„œ ë³€ê²½)
   ì˜ˆì‹œ: "ë‚˜ì™”ë‚˜ìš”" â†’ "ë‚˜ì™”ìš”ë‚˜" (ë‚˜ìš” ìˆœì„œ ë³€ê²½)

5. ë„ì–´ì“°ê¸° ì˜¤ë¥˜(Spacing): ê³µë°± ì¶”ê°€/ì œê±°
   - ê³µë°± ì œê±°: "ì–´ë””ì—ì„œ ë‚˜ì™”ë‚˜ìš”" â†’ "ì–´ë””ì—ì„œë‚˜ì™”ë‚˜ìš”"
   - ê³µë°± ì¶”ê°€: "ìŠ¤íƒ€ë²…ìŠ¤" â†’ "ìŠ¤íƒ€ ë²…ìŠ¤"
   ì˜ˆì‹œ: "ê·¸ë“¤ë§Œì˜ ë¦¬ê·¸" â†’ "ê·¸ë“¤ë§Œì˜ë¦¬ê·¸" (ê³µë°± ì œê±°)
   ì˜ˆì‹œ: "ë¼ì´ì˜¨í‚¹" â†’ "ë¼ì´ì˜¨ í‚¹" (ê³µë°± ì¶”ê°€)

ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ 3ê°œ ë²„ì „ ìƒì„±:
- 0 errors: ì˜¤íƒ€ ì—†ìŒ (ì›ë³¸ ê·¸ëŒ€ë¡œ)
- 1 error: í•´ë‹¹ ì˜¤íƒ€ ìœ í˜• 1ë²ˆ ì ìš©
- 2 errors: 1 error ë²„ì „ì— ì¶”ê°€ë¡œ 1ê°œ ë” ì ìš© (ëˆ„ì )

ì¤‘ìš” ì§€ì¹¨:
- ì˜¤íƒ€ëŠ” ëª…í™•í•˜ê²Œ êµ¬ë¶„ ê°€ëŠ¥í•´ì•¼ í•¨ (ë¯¸ë¬˜í•œ ë³€í™” X)
- 2ê°œ ì˜¤íƒ€ëŠ” 1ê°œ ì˜¤íƒ€ ë²„ì „ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ê°€ (ëˆ„ì  ë°©ì‹)
- ê° ë‹¨ê³„ë§ˆë‹¤ ì°¨ì´ê°€ í™•ì‹¤íˆ ë³´ì—¬ì•¼ í•¨
- ì˜ˆì‹œ: "ìŠ¤íƒ€ë²…ìŠ¤ ë¡œê³ ëŠ” ì–´ë””ì—ì„œ ë‚˜ì™”ë‚˜ìš”"
  - 0 errors: "ìŠ¤íƒ€ë²…ìŠ¤ ë¡œê³ ëŠ” ì–´ë””ì—ì„œ ë‚˜ì™”ë‚˜ìš”" (ì›ë³¸)
  - 1 error: "ìŠ¤íƒ€ë²…ìŠ¤ ë¡œê³ ëŠ” ì–´ë””ì• ì„œ ë‚˜ì™”ë‚˜ìš”" (ã…”â†’ã…)
  - 2 errors: "ìŠ¤í„°ë²…ìŠ¤ ë¡œê³ ëŠ” ì–´ë””ì• ì„œ ë‚˜ì™”ë‚˜ìš”" (1 error ë²„ì „ì—ì„œ ã…â†’ã…“ ì¶”ê°€)

ëˆ„ì  ë°©ì‹ ì„¤ëª…:
- 1 error ë²„ì „ì„ ë¨¼ì € ë§Œë“¤ê³ 
- 2 errors ë²„ì „ì€ 1 error ë²„ì „ì—ì„œ ë‹¤ë¥¸ ìœ„ì¹˜ì— ì˜¤íƒ€ë¥¼ í•˜ë‚˜ ë” ì¶”ê°€
- ì´ë ‡ê²Œ í•˜ë©´ ì˜¤íƒ€ê°€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

ì…ë ¥ ë°ì´í„°:
{batch_text}

ì¶œë ¥ í˜•ì‹ (ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ ìƒì„±):
**ë°˜ë“œì‹œ ëˆ„ì  ë°©ì‹ìœ¼ë¡œ ìƒì„±**
[
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "error_type": "substitution",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0}},
      {{"text": "ì›ë³¸ì— êµì²´ ì˜¤íƒ€ 1ê°œ ì ìš©", "num_errors": 1}},
      {{"text": "ìœ„ì˜ 1 error ë²„ì „ì— êµì²´ ì˜¤íƒ€ 1ê°œ ë” ì¶”ê°€", "num_errors": 2}}
    ]
  }},
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "error_type": "deletion",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0}},
      {{"text": "ì›ë³¸ì— ì‚­ì œ ì˜¤íƒ€ 1ê°œ ì ìš©", "num_errors": 1}},
      {{"text": "ìœ„ì˜ 1 error ë²„ì „ì— ì‚­ì œ ì˜¤íƒ€ 1ê°œ ë” ì¶”ê°€", "num_errors": 2}}
    ]
  }},
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "error_type": "insertion",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0}},
      {{"text": "ì›ë³¸ì— ì¶”ê°€ ì˜¤íƒ€ 1ê°œ ì ìš©", "num_errors": 1}},
      {{"text": "ìœ„ì˜ 1 error ë²„ì „ì— ì¶”ê°€ ì˜¤íƒ€ 1ê°œ ë” ì¶”ê°€", "num_errors": 2}}
    ]
  }},
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "error_type": "transposition",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0}},
      {{"text": "ì›ë³¸ì— ì „ì¹˜ ì˜¤íƒ€ 1ê°œ ì ìš©", "num_errors": 1}},
      {{"text": "ìœ„ì˜ 1 error ë²„ì „ì— ì „ì¹˜ ì˜¤íƒ€ 1ê°œ ë” ì¶”ê°€", "num_errors": 2}}
    ]
  }},
  {{
    "en": "ì˜ì–´ ì§ˆë¬¸",
    "ko_original": "ì›ë³¸ í•œêµ­ì–´",
    "error_type": "spacing",
    "variants": [
      {{"text": "ì›ë³¸ ê·¸ëŒ€ë¡œ", "num_errors": 0}},
      {{"text": "ì›ë³¸ì— ë„ì–´ì“°ê¸° ì˜¤íƒ€ 1ê°œ ì ìš©", "num_errors": 1}},
      {{"text": "ìœ„ì˜ 1 error ë²„ì „ì— ë„ì–´ì“°ê¸° ì˜¤íƒ€ 1ê°œ ë” ì¶”ê°€", "num_errors": 2}}
    ]
  }}
]

JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í•œêµ­ì–´ ì˜¤íƒ€ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ì˜¤íƒ€ ìœ í˜•ë³„ë¡œ ëª…í™•í•˜ê³  êµ¬ë¶„ ê°€ëŠ¥í•œ ì˜¤íƒ€ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )

        # ì‘ë‹µ íŒŒì‹±
        result_text = response.choices[0].message.content.strip()

        # JSON ì¶”ì¶œ
        if result_text.startswith("```json"):
            result_text = result_text[7:]
        if result_text.startswith("```"):
            result_text = result_text[3:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]

        result = json.loads(result_text)
        return result

    except json.JSONDecodeError as e:
        print(f"âŒ JSON parsing error: {e}")
        print(f"Response: {result_text[:200]}...")
        return []
    except Exception as e:
        print(f"âŒ API error: {e}")
        return []

def process_dataset(input_file: str, output_file: str, batch_size: int = 5):
    """ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ì˜¤íƒ€ ìƒì„±"""

    print(f"ğŸ“š Loading {input_file}...")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ë°ì´í„° í˜•ì‹ í™•ì¸ ë° ë³€í™˜
    simple_data = []
    for entry in data:
        if "query" in entry:
            simple_data.append({
                "en": entry["query"],
                "ko": entry["queries"]["ko"]
            })
        else:
            simple_data.append(entry)

    print(f"ğŸ“Š Total entries to process: {len(simple_data)}")

    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = initialize_client()

    # ë°°ì¹˜ ì²˜ë¦¬
    all_results = []
    error_types = ['substitution', 'deletion', 'insertion', 'transposition', 'spacing']

    print(f"ğŸ”„ Processing in batches of {batch_size}...")

    for i in tqdm(range(0, len(simple_data), batch_size), desc="Processing batches"):
        batch = simple_data[i:i+batch_size]

        # GPTë¡œ ì˜¤íƒ€ ìƒì„±
        batch_results = generate_typos_batch(client, batch, batch_size)

        # ê²°ê³¼ë¥¼ í”Œë« í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for entry_group in batch_results:
            en_query = entry_group.get("en", "")
            ko_original = entry_group.get("ko_original", "")
            error_type = entry_group.get("error_type", "")

            for variant in entry_group.get("variants", []):
                flat_entry = {
                    "en": en_query,
                    "ko_original": ko_original,
                    "ko_typo": variant["text"],
                    "error_type": error_type,
                    "num_errors": variant["num_errors"],
                    "applied_errors": [error_type] * variant["num_errors"] if variant["num_errors"] > 0 else []
                }
                all_results.append(flat_entry)

        # Rate limiting
        if i + batch_size < len(simple_data):
            time.sleep(1)

    print(f"\nâœ… Generated {len(all_results)} entries")

    # ê²°ê³¼ ì €ì¥
    print(f"ğŸ’¾ Saving to {output_file}...")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š Statistics:")
    print(f"  Original entries: {len(simple_data)}")
    print(f"  Generated entries: {len(all_results)}")

    # ê° ì˜¤íƒ€ ìœ í˜•ë³„ ìƒ˜í”Œ ì¶œë ¥
    print("\nğŸ“ Sample outputs by error type:")
    for error_type in error_types:
        type_entries = [e for e in all_results if e['error_type'] == error_type]
        if len(type_entries) >= 3:
            print(f"\n--- {error_type.upper()} ---")
            print(f"Original: {type_entries[0]['ko_original']}")
            print(f"0 errors: {type_entries[0]['ko_typo']}")
            print(f"1 error:  {type_entries[1]['ko_typo']}")
            print(f"2 errors: {type_entries[2]['ko_typo']}")

    return len(all_results)

def create_test_sample(input_file: str, sample_file: str, sample_size: int = 5):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ íŒŒì¼ ìƒì„±"""
    print(f"ğŸ“ Creating sample file with {sample_size} entries...")

    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    sample_data = data[:sample_size]

    with open(sample_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… Sample file created: {sample_file}")
    return sample_file

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate typos using GPT-4o-mini")
    parser.add_argument("--input", default="mkqa_formatted.json", help="Input JSON file")
    parser.add_argument("--output", default="mkqa_with_typos_gpt.json", help="Output JSON file")
    parser.add_argument("--batch-size", type=int, default=3, help="Batch size for API calls")
    parser.add_argument("--test", action="store_true", help="Test with small sample first")
    parser.add_argument("--sample-size", type=int, default=5, help="Sample size for testing")

    args = parser.parse_args()

    if args.test:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        sample_file = "mkqa_typo_sample.json"
        create_test_sample(args.input, sample_file, args.sample_size)

        print("\nğŸ§ª Testing with sample file...")
        count = process_dataset(
            sample_file,
            "mkqa_typo_sample_output.json",
            batch_size=2
        )
        print(f"\nâœ… Test complete! Generated {count} entries")
        print("Check 'mkqa_typo_sample_output.json' for results")
    else:
        # ì „ì²´ ì²˜ë¦¬
        count = process_dataset(
            args.input,
            args.output,
            batch_size=args.batch_size
        )
        print(f"\nâœ… Complete! Generated {count} entries")
        print(f"Output saved to: {args.output}")